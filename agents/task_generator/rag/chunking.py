"""
============================================
RAG CHUNKING & LOADING SYSTEM
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î (FAISS)
============================================
"""

import os
from typing import List, Dict, Any
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings

# ============================================
# –ü–ê–†–ê–ú–ï–¢–†–´ –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ
# ============================================

DEFAULT_CHUNK_SIZE = 1000  # –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (—Å–∏–º–≤–æ–ª–æ–≤)
DEFAULT_CHUNK_OVERLAP = 200  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ (—Å–∏–º–≤–æ–ª–æ–≤)
DEFAULT_SEPARATORS = [
    "\n\n",  # –î–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å—Ç—Ä–æ–∫–∏ (–∞–±–∑–∞—Ü)
    "\n",  # –û–¥–∏–Ω –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏ (—Å—Ç—Ä–æ–∫–∞)
    ". ",  # –ö–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    " ",  # –ü—Ä–æ–±–µ–ª
    ",",  # –ó–∞–ø—è—Ç–∞—è
    ""  # –°–∏–º–≤–æ–ª (–µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ)
]


# ============================================
# 1. –°–û–ó–î–ê–ù–ò–ï –°–ü–õ–ò–¢–¢–ï–†–ê
# ============================================

def create_text_splitter(
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
        separators: List[str] = None
) -> RecursiveCharacterTextSplitter:
    """
    –°–æ–∑–¥–∞—ë—Ç —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏.

    Args:
        chunk_size: –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        chunk_overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        separators: –°–ø–∏—Å–æ–∫ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è

    Returns:
        RecursiveCharacterTextSplitter –æ–±—ä–µ–∫—Ç

    –ü—Ä–∏–º–µ—Ä:
        splitter = create_text_splitter(chunk_size=500, chunk_overlap=100)
    """
    if separators is None:
        separators = DEFAULT_SEPARATORS

    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators,
        length_function=len,
    )


# ============================================
# 2. –†–ê–ó–ë–ò–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ù–ê –ß–ê–ù–ö–ò
# ============================================

def split_text_into_chunks(
        text: str,
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
        separators: List[str] = None
) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        chunk_size: –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        chunk_overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        separators: –°–µ–ø–∞—Ä–∞—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è

    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ (—Å—Ç—Ä–æ–∫)

    –ü—Ä–∏–º–µ—Ä:
        chunks = split_text_into_chunks("–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç...", chunk_size=500)
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    """
    splitter = create_text_splitter(chunk_size, chunk_overlap, separators)

    # –û–±–≤–æ—Ä–∞—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ Document –æ–±—ä–µ–∫—Ç (–∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç LangChain)
    from langchain_core.documents import Document
    documents = [Document(page_content=text)]

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    split_docs = splitter.split_documents(documents)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    chunks = [doc.page_content for doc in split_docs]

    return chunks


# ============================================
# 3. –°–û–ó–î–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
# ============================================

def create_embeddings(
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
) -> SentenceTransformerEmbeddings:
    """
    –°–æ–∑–¥–∞—ë—Ç –æ–±—ä–µ–∫—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

    Args:
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ SentenceTransformer

    Returns:
        SentenceTransformerEmbeddings –æ–±—ä–µ–∫—Ç

    –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏:
        - "sentence-transformers/all-MiniLM-L6-v2" (–±—ã—Å—Ç—Ä–∞—è, –ª–µ–≥–∫–∞—è)
        - "sentence-transformers/all-mpnet-base-v2" (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è)
        - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2" (–º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è)
    """
    return SentenceTransformerEmbeddings(model_name=model_name)


# ============================================
# 4. –ó–ê–ì–†–£–ó–ö–ê –í FAISS (–í–ï–ö–¢–û–†–ù–ê–Ø –ë–î)
# ============================================

def load_documents_to_faiss(
        text: str,
        db_path: str,
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
        separators: List[str] = None,
        embeddings=None,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
) -> FAISS:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ FAISS.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏—è
        db_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ë–î (–Ω–∞–ø—Ä–∏–º–µ—Ä: "vector_db/my_data")
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        separators: –°–µ–ø–∞—Ä–∞—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
        embeddings: –û–±—ä–µ–∫—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞—ë—Ç—Å—è –Ω–æ–≤—ã–π)
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

    Returns:
        FAISS –æ–±—ä–µ–∫—Ç (–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∏ –≥–æ—Ç–æ–≤—ã–π –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é)

    –ü—Ä–∏–º–µ—Ä:
        vectorstore = load_documents_to_faiss(
            text="–ú–æ–π —Ç–µ–∫—Å—Ç...",
            db_path="vector_db/task_generation_faiss",
            chunk_size=1000
        )
    """
    print("üìÑ –†–∞–∑–±–∏–≤–∞—é —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏...")
    chunks = split_text_into_chunks(text, chunk_size, chunk_overlap, separators)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_chars = sum(len(chunk) for chunk in chunks)
    avg_size = total_chars // len(chunks) if chunks else 0
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {avg_size} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   ‚Ä¢ –ü—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_chars // 4}")

    # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–ª–∏
    if embeddings is None:
        print(f"\nüßÆ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
        embeddings = create_embeddings(model_name)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∞–Ω–∫–∏ –≤ Document –æ–±—ä–µ–∫—Ç—ã
    from langchain_core.documents import Document
    documents = [Document(page_content=chunk) for chunk in chunks]

    # –°–æ–∑–¥–∞—ë–º FAISS –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("üóÇÔ∏è  –°–æ–∑–¥–∞—é FAISS –∏–Ω–¥–µ–∫—Å...")
    vectorstore = FAISS.from_documents(documents, embeddings)

    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(db_path, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ë–î
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –ë–î –≤ {db_path}")
    vectorstore.save_local(db_path)

    print("‚úÖ –ì–æ—Ç–æ–≤–æ! FAISS –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\n")

    return vectorstore


# ============================================
# 5. –ó–ê–ì–†–£–ó–ö–ê –°–£–©–ï–°–¢–í–£–Æ–©–ï–ô FAISS –ë–î
# ============================================

def load_faiss_vectorstore(
        db_path: str,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
) -> FAISS:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é FAISS –ë–î.

    Args:
        db_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –ë–î
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

    Returns:
        FAISS –æ–±—ä–µ–∫—Ç

    –ü—Ä–∏–º–µ—Ä:
        vectorstore = load_faiss_vectorstore("vector_db/task_generation_faiss")
    """
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é FAISS –∏–∑ {db_path}")

    embeddings = create_embeddings(model_name)
    vectorstore = FAISS.load_local(
        db_path,
        embeddings,
        allow_dangerous_deserialization=True
    )

    print("‚úÖ FAISS –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\n")
    return vectorstore


# ============================================
# 6. –ü–û–ò–°–ö –í FAISS (RAG)
# ============================================

def search_similar_chunks(
        vectorstore: FAISS,
        query: str,
        k: int = 5
) -> List[Dict[str, Any]]:
    """
    –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ –≤ FAISS.

    Args:
        vectorstore: FAISS –æ–±—ä–µ–∫—Ç
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

    –ü—Ä–∏–º–µ—Ä:
        results = search_similar_chunks(vectorstore, "–§—É–Ω–∫—Ü–∏–∏ –≤ Python", k=3)
        for result in results:
            print(result['content'])
    """
    results = vectorstore.similarity_search(query, k=k)

    formatted_results = [
        {
            'content': doc.page_content,
            'metadata': doc.metadata if hasattr(doc, 'metadata') else {}
        }
        for doc in results
    ]

    return formatted_results


# ============================================
# 7. –î–û–ë–ê–í–õ–ï–ù–ò–ï –ù–û–í–´–• –î–û–ö–£–ú–ï–ù–¢–û–í
# ============================================

def add_documents_to_faiss(
        vectorstore: FAISS,
        new_texts: List[str],
        db_path: str
) -> FAISS:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é FAISS –ë–î.

    Args:
        vectorstore: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π FAISS –æ–±—ä–µ–∫—Ç
        new_texts: –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤/—á–∞–Ω–∫–æ–≤
        db_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –ë–î

    Returns:
        –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π FAISS –æ–±—ä–µ–∫—Ç

    –ü—Ä–∏–º–µ—Ä:
        new_chunks = ["–ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç 1", "–ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç 2"]
        vectorstore = add_documents_to_faiss(vectorstore, new_chunks, "vector_db/my_data")
    """
    from langchain_core.documents import Document

    print(f"‚ûï –î–æ–±–∞–≤–ª—è—é {len(new_texts)} –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

    documents = [Document(page_content=text) for text in new_texts]
    vectorstore.add_documents(documents)

    os.makedirs(db_path, exist_ok=True)
    vectorstore.save_local(db_path)

    print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏ –ë–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {db_path}\n")

    return vectorstore


# ============================================
# 8. –£–¢–ò–õ–ò–¢–ê: –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ë–î
# ============================================

def get_vectorstore_info(vectorstore: FAISS) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ FAISS –ë–î.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ë–î
    """
    try:
        index_size = vectorstore.index.ntotal
        return {
            'total_documents': index_size,
            'index_type': type(vectorstore.index).__name__,
            'dimension': vectorstore.index.d if hasattr(vectorstore.index, 'd') else 'Unknown'
        }
    except:
        return {'status': 'Unable to retrieve info'}
